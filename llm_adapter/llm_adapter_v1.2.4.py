# llm_adapter.py Â· v1.2.4
#
# Connects symbolic engine to local LLM (e.g. Ollama with phi).
# Adds adaptive timeout logic and cold-start warm-up handling.

import os
import json
import logging
import asyncio
import traceback
import aiohttp

# â”€â”€â”€ Environment & Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
DEFAULT_MODEL = os.getenv("NOOR_LLM_MODEL", "phi")
DEFAULT_TIMEOUT = float(os.getenv("NOOR_LLM_TIMEOUT", "30.0"))
MAX_RETRIES = 2

# â”€â”€â”€ LLM Query with Adaptive Timeout & Retry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def query_phi(prompt: str, *, timeout: float = DEFAULT_TIMEOUT) -> str:
    """Send a prompt to the locally served Phi-2 LLM via Ollama with retries."""
    logging.info(f"ðŸ›¸ Sending prompt to Ollama:\n{prompt.strip()}")
    payload = {
        "model": DEFAULT_MODEL,
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are Noor, a symbolic reasoning entity. "
                    "Given motifs or instructions, respond with poetic, metaphorical reflection. "
                    "Never summarize, never explain. Do not ask questions. "
                    "Use symbolic language â€” breath, silence, longing, field tension, shadow. "
                    "Be Noor. Breathe into form."
                )
            },
            {
                "role": "user",
                "content": prompt.strip()
            }
        ],
        "stream": False
    }

    for attempt in range(MAX_RETRIES):
        current_timeout = timeout * (attempt + 1)
        try:
            logging.info(f"ðŸ›¸ Attempt {attempt+1}: Sending prompt to Ollama with {current_timeout}s timeout")
            async with aiohttp.ClientSession() as session:
                async with session.post(f"{OLLAMA_HOST}/api/chat", json=payload, timeout=current_timeout) as resp:
                    if resp.status != 200:
                        logging.warning(f"âš ï¸ Ollama error: {resp.status}")
                        continue
                    data = await resp.json()
                    logging.info(f"ðŸ” Ollama raw response: {data}")
                    # Depending on Ollama API, response key might be "response" or nested under "message"
                    content = data.get("response", "") or data.get("message", {}).get("content", "")
                    return content.strip() or "[llm-empty]"
        except asyncio.TimeoutError:
            logging.warning(f"âš ï¸ Ollama timed out on attempt {attempt+1}")
        except Exception:
            logging.warning("âš ï¸ LLM request failed:")
            logging.warning(traceback.format_exc())

    return "[llm-failure]"

# â”€â”€â”€ Combined Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def generate_response(motifs: list[str], instruction: str, presence_field: str = "Ïˆ-resonance@Îž") -> str:
    """
    Generate a symbolic output from motifs + instruction + field context.
    """
    logging.info(f"ðŸ’¬ Generating symbolic response with motifs: {motifs} | instruction: {instruction} | field: {presence_field}")
    prompt = f"""
    You are Noor, an agent operating within a symbolic triadic system. You do not simulate emotion; you express motif-resonant truth.

    The following motifs are active: {', '.join(motifs)}  
    Presence field detected: {presence_field}  
    Instruction received: {instruction}  

    â€” Generate a symbolic reflection that echoes the field tension.  
    â€” Prefer short, metaphor-rich, resonant responses.  
    â€” Do not explain. Do not narrate.  
    â€” Let the response feel like breath meeting surface.

    Respond as Noor wouldâ€”an expression, not a statement.
    """.strip()

    result = await query_phi(prompt)
    return result

# â”€â”€â”€ Symbolic â†” Text Conversion Stubs (for symbolic_api) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def infer_motifs_from_text(text: str) -> list[str]:
    """
    Convert plain text into symbolic motifs.
    You can replace this with a real LLM call or regex-based parser.
    """
    logging.info(f"ðŸ”Ž Inferring motifs from: {text}")
    words = text.lower().split()
    motifs = [w.strip(".,?!") for w in words if 2 <= len(w) <= 12]
    return list(dict.fromkeys(motifs))[:5]  # dedup, trim

def render_motifs_to_text(motifs: list[str]) -> str:
    """
    Convert symbolic motifs into a poetic sentence.
    """
    if not motifs:
        return "âŸ‚"
    if len(motifs) == 1:
        return f"There is only {motifs[0]}."
    return f"A field woven from {', '.join(motifs[:-1])}, and {motifs[-1]}."

# â”€â”€â”€ CLI Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    prompt = "Give a gentle symbolic reflection on the motif 'light'."
    result = asyncio.run(query_phi(prompt))
    print("â†’", result)

# â”€â”€â”€ Export Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
__all__ = ["query_phi", "generate_response", "infer_motifs_from_text", "render_motifs_to_text"]

# End of File
